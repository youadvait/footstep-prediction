{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "196d4d1b-c540-4c96-aebb-a1ecae333d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß PHASE 1 FIXED: Split FIRST, Then Augment\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 3.0\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"üîß PHASE 1 FIXED: Split FIRST, Then Augment\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ef422b2-ac8b-4104-bd26-784ecb856cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Original Files Found:\n",
      "   Footstep files: 21\n",
      "   Non-footstep files: 103\n"
     ]
    }
   ],
   "source": [
    "# Same file discovery as before\n",
    "base_path = Path('footstepData')\n",
    "\n",
    "non_footstep_folders = [\n",
    "    base_path / 'Bo6GunSounds' / 'GunReloading',\n",
    "    base_path / 'Bo6GunSounds' / 'Gunshot Sounds', \n",
    "    base_path / 'UselessSoundPack'\n",
    "]\n",
    "\n",
    "footstep_folder = base_path / 'FootstepSounds'\n",
    "excluded_folder = footstep_folder / 'Gun+Footsteppack'\n",
    "\n",
    "def list_audio_files(folders, exclude=None):\n",
    "    files = []\n",
    "    supported_formats = ['.mp4', '.wav', '.mp3', '.m4a', '.flac']\n",
    "    \n",
    "    for folder in folders:\n",
    "        if folder.exists():\n",
    "            for file in folder.rglob('*'):\n",
    "                if file.suffix.lower() in supported_formats:\n",
    "                    if exclude and exclude in file.parents:\n",
    "                        continue\n",
    "                    files.append(file)\n",
    "    return files\n",
    "\n",
    "footstep_files = list_audio_files([footstep_folder], exclude=excluded_folder)\n",
    "non_footstep_files = list_audio_files(non_footstep_folders)\n",
    "\n",
    "print(f\"üìä Original Files Found:\")\n",
    "print(f\"   Footstep files: {len(footstep_files)}\")\n",
    "print(f\"   Non-footstep files: {len(non_footstep_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b0c53a6-4b32-405e-acfb-cae667f1e250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ STEP 1: SPLITTING ORIGINAL FILES (NO AUGMENTATION YET)\n",
      "============================================================\n",
      "‚úÖ Original File Splits (LEAK-FREE):\n",
      "   Train: 14 footstep, 72 non-footstep\n",
      "   Val: 3 footstep, 15 non-footstep\n",
      "   Test: 4 footstep, 16 non-footstep\n",
      "   Overlap check: ‚úÖ NO LEAKAGE\n"
     ]
    }
   ],
   "source": [
    "print(\"üéØ STEP 1: SPLITTING ORIGINAL FILES (NO AUGMENTATION YET)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Split footstep files\n",
    "footstep_train_files, footstep_temp = train_test_split(\n",
    "    footstep_files, test_size=0.3, random_state=RANDOM_STATE\n",
    ")\n",
    "footstep_val_files, footstep_test_files = train_test_split(\n",
    "    footstep_temp, test_size=0.5, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Split non-footstep files\n",
    "non_footstep_train_files, non_footstep_temp = train_test_split(\n",
    "    non_footstep_files, test_size=0.3, random_state=RANDOM_STATE\n",
    ")\n",
    "non_footstep_val_files, non_footstep_test_files = train_test_split(\n",
    "    non_footstep_temp, test_size=0.5, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Original File Splits (LEAK-FREE):\")\n",
    "print(f\"   Train: {len(footstep_train_files)} footstep, {len(non_footstep_train_files)} non-footstep\")\n",
    "print(f\"   Val: {len(footstep_val_files)} footstep, {len(non_footstep_val_files)} non-footstep\")\n",
    "print(f\"   Test: {len(footstep_test_files)} footstep, {len(non_footstep_test_files)} non-footstep\")\n",
    "\n",
    "# Verify no overlap\n",
    "train_stems = {f.stem for f in footstep_train_files + non_footstep_train_files}\n",
    "val_stems = {f.stem for f in footstep_val_files + non_footstep_val_files}\n",
    "test_stems = {f.stem for f in footstep_test_files + non_footstep_test_files}\n",
    "\n",
    "overlap = train_stems.intersection(val_stems) or train_stems.intersection(test_stems) or val_stems.intersection(test_stems)\n",
    "print(f\"   Overlap check: {'‚úÖ NO LEAKAGE' if not overlap else '‚ùå STILL HAVE LEAKAGE'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "135fc2c2-ed41-49d7-9c4f-b59536dbc97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Augmentation functions defined\n"
     ]
    }
   ],
   "source": [
    "def segment_and_augment_file(file_path, target_samples=50):\n",
    "    \"\"\"Process single file with segmentation and augmentation\"\"\"\n",
    "    try:\n",
    "        audio, sr = librosa.load(str(file_path), sr=SAMPLE_RATE, mono=True)\n",
    "        segments = []\n",
    "        \n",
    "        # Segment audio (3-second chunks with overlap)\n",
    "        target_length = int(SAMPLE_RATE * DURATION)\n",
    "        if len(audio) < target_length:\n",
    "            audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "        \n",
    "        # Create overlapping segments\n",
    "        hop_length = target_length // 2  # 50% overlap\n",
    "        for start in range(0, len(audio) - target_length + 1, hop_length):\n",
    "            segment = audio[start:start + target_length]\n",
    "            segments.append(segment)\n",
    "        \n",
    "        # Apply augmentation to segments\n",
    "        augmented_samples = []\n",
    "        for segment in segments:\n",
    "            # Original\n",
    "            augmented_samples.append(segment)\n",
    "            \n",
    "            # Pitch shifts\n",
    "            for n_steps in [-1, 1]:\n",
    "                try:\n",
    "                    pitched = librosa.effects.pitch_shift(segment, sr=sr, n_steps=n_steps)\n",
    "                    augmented_samples.append(pitched)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Time stretch\n",
    "            for rate in [0.9, 1.1]:\n",
    "                try:\n",
    "                    stretched = librosa.effects.time_stretch(segment, rate=rate)\n",
    "                    if len(stretched) > len(segment):\n",
    "                        stretched = stretched[:len(segment)]\n",
    "                    else:\n",
    "                        stretched = np.pad(stretched, (0, len(segment) - len(stretched)))\n",
    "                    augmented_samples.append(stretched)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Noise\n",
    "            try:\n",
    "                noise = np.random.normal(0, 0.005, len(segment))\n",
    "                noisy = segment + noise\n",
    "                augmented_samples.append(noisy)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Limit to target number of samples\n",
    "        if len(augmented_samples) > target_samples:\n",
    "            augmented_samples = augmented_samples[:target_samples]\n",
    "        \n",
    "        return augmented_samples\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"‚úÖ Augmentation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb6b9da-9faf-4910-a506-66038ae6e836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ PROCESSING SPLITS SEPARATELY (NO LEAKAGE)\n",
      "==================================================\n",
      "üîß Processing train split: 14 files...\n",
      "   Processed 5/14 files...\n",
      "   Processed 10/14 files...\n",
      "   ‚úÖ Generated 274 samples for train\n",
      "üîß Processing train split: 72 files...\n",
      "   Processed 5/72 files...\n",
      "   Processed 10/72 files...\n",
      "   Processed 15/72 files...\n",
      "   Processed 20/72 files...\n",
      "   Processed 25/72 files...\n",
      "   Processed 30/72 files...\n",
      "   Processed 35/72 files...\n",
      "   Processed 40/72 files...\n",
      "   Processed 45/72 files...\n",
      "   Processed 50/72 files...\n",
      "   Processed 55/72 files...\n",
      "   Processed 60/72 files...\n",
      "   Processed 65/72 files...\n",
      "   Processed 70/72 files...\n",
      "   ‚úÖ Generated 833 samples for train\n",
      "üîß Processing val split: 3 files...\n",
      "   ‚úÖ Generated 48 samples for val\n",
      "üîß Processing val split: 15 files...\n",
      "   Processed 5/15 files...\n",
      "   Processed 10/15 files...\n",
      "   Processed 15/15 files...\n",
      "   ‚úÖ Generated 189 samples for val\n",
      "üîß Processing test split: 4 files...\n",
      "   ‚úÖ Generated 60 samples for test\n",
      "üîß Processing test split: 16 files...\n",
      "   Processed 5/16 files...\n",
      "   Processed 10/16 files...\n",
      "   Processed 15/16 files...\n",
      "   ‚úÖ Generated 169 samples for test\n",
      "\n",
      "üìä FINAL LEAK-FREE DATASET:\n",
      "   Training: 1107 samples\n",
      "   Validation: 237 samples\n",
      "   Test: 229 samples\n"
     ]
    }
   ],
   "source": [
    "def process_file_split(files, label, split_name, target_samples_per_file=30):\n",
    "    \"\"\"Process a single split with augmentation\"\"\"\n",
    "    print(f\"üîß Processing {split_name} split: {len(files)} files...\")\n",
    "    \n",
    "    all_samples = []\n",
    "    \n",
    "    for i, file_path in enumerate(files):\n",
    "        samples = segment_and_augment_file(file_path, target_samples_per_file)\n",
    "        \n",
    "        for j, sample in enumerate(samples):\n",
    "            all_samples.append({\n",
    "                'audio_data': sample,\n",
    "                'label': label,\n",
    "                'class_name': 'footstep' if label == 1 else 'non_footstep',\n",
    "                'original_file': str(file_path),\n",
    "                'sample_id': f\"{file_path.stem}_{split_name}_{j:03d}\"\n",
    "            })\n",
    "        \n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"   Processed {i + 1}/{len(files)} files...\")\n",
    "    \n",
    "    print(f\"   ‚úÖ Generated {len(all_samples)} samples for {split_name}\")\n",
    "    return all_samples\n",
    "\n",
    "# Process each split separately (THIS IS THE KEY FIX)\n",
    "print(\"üöÄ PROCESSING SPLITS SEPARATELY (NO LEAKAGE)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Training set\n",
    "train_footstep_samples = process_file_split(footstep_train_files, 1, 'train', 35)\n",
    "train_non_footstep_samples = process_file_split(non_footstep_train_files, 0, 'train', 25)\n",
    "train_samples = train_footstep_samples + train_non_footstep_samples\n",
    "\n",
    "# Validation set  \n",
    "val_footstep_samples = process_file_split(footstep_val_files, 1, 'val', 35)\n",
    "val_non_footstep_samples = process_file_split(non_footstep_val_files, 0, 'val', 25)\n",
    "val_samples = val_footstep_samples + val_non_footstep_samples\n",
    "\n",
    "# Test set\n",
    "test_footstep_samples = process_file_split(footstep_test_files, 1, 'test', 35) \n",
    "test_non_footstep_samples = process_file_split(non_footstep_test_files, 0, 'test', 25)\n",
    "test_samples = test_footstep_samples + test_non_footstep_samples\n",
    "\n",
    "print(f\"\\nüìä FINAL LEAK-FREE DATASET:\")\n",
    "print(f\"   Training: {len(train_samples)} samples\")\n",
    "print(f\"   Validation: {len(val_samples)} samples\") \n",
    "print(f\"   Test: {len(test_samples)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fa45995-d569-4225-85d5-9eadf12e70e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SAVED LEAK-FREE DATASETS:\n",
      "   train_manifest_FIXED.pkl (1107 samples)\n",
      "   val_manifest_FIXED.pkl (237 samples)\n",
      "   test_manifest_FIXED.pkl (229 samples)\n",
      "\n",
      "üéØ Class Distribution:\n",
      "   Train: 274 footstep, 833 non-footstep (ratio: 0.33)\n",
      "   Val: 48 footstep, 189 non-footstep (ratio: 0.25)\n",
      "   Test: 60 footstep, 169 non-footstep (ratio: 0.35)\n",
      "\n",
      "üöÄ READY FOR PHASE 2 WITH FIXED DATA!\n",
      "   Expected model performance: 75-90% (realistic)\n",
      "   No more perfect scores from data leakage\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrames and save\n",
    "train_df = pd.DataFrame(train_samples).sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "val_df = pd.DataFrame(val_samples).sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "test_df = pd.DataFrame(test_samples).sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "# Save to pickle (preserves audio data)\n",
    "train_df.to_pickle('train_manifest_FIXED.pkl')\n",
    "val_df.to_pickle('val_manifest_FIXED.pkl')\n",
    "test_df.to_pickle('test_manifest_FIXED.pkl')\n",
    "\n",
    "print(f\"‚úÖ SAVED LEAK-FREE DATASETS:\")\n",
    "print(f\"   train_manifest_FIXED.pkl ({len(train_df)} samples)\")\n",
    "print(f\"   val_manifest_FIXED.pkl ({len(val_df)} samples)\")\n",
    "print(f\"   test_manifest_FIXED.pkl ({len(test_df)} samples)\")\n",
    "\n",
    "print(f\"\\nüéØ Class Distribution:\")\n",
    "for split_name, df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
    "    footstep_count = len(df[df['label'] == 1])\n",
    "    non_footstep_count = len(df[df['label'] == 0])\n",
    "    ratio = footstep_count / (non_footstep_count + 1)\n",
    "    print(f\"   {split_name}: {footstep_count} footstep, {non_footstep_count} non-footstep (ratio: {ratio:.2f})\")\n",
    "\n",
    "print(f\"\\nüöÄ READY FOR PHASE 2 WITH FIXED DATA!\")\n",
    "print(f\"   Expected model performance: 75-90% (realistic)\")\n",
    "print(f\"   No more perfect scores from data leakage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c25573-12b5-4aea-aeb4-82bd3c26c214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f4192a3-7e4a-4cd0-b39e-c6840716a811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéµ Phase 2: Feature Extraction (LEAK-FREE)\n",
      "==================================================\n",
      "üìä Configuration:\n",
      "   Sample Rate: 16000 Hz\n",
      "   Duration: 3.0 seconds\n",
      "   MFCC coefficients: 13\n",
      "   Mel filters: 128\n",
      "   Hop length: 512\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Feature Extraction (Leak-Free)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Feature extraction configuration\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 3.0\n",
    "N_MFCC = 13\n",
    "N_MELS = 128\n",
    "HOP_LENGTH = 512\n",
    "N_FFT = 2048\n",
    "\n",
    "print(\"üéµ Phase 2: Feature Extraction (LEAK-FREE)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Configuration:\")\n",
    "print(f\"   Sample Rate: {SAMPLE_RATE} Hz\")\n",
    "print(f\"   Duration: {DURATION} seconds\")\n",
    "print(f\"   MFCC coefficients: {N_MFCC}\")\n",
    "print(f\"   Mel filters: {N_MELS}\")\n",
    "print(f\"   Hop length: {HOP_LENGTH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c569a9c0-14ed-43c7-b87a-7209f670f1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading leak-free datasets...\n",
      "‚úÖ Loaded leak-free datasets:\n",
      "   Training: 1107 samples\n",
      "   Validation: 237 samples\n",
      "   Test: 229 samples\n",
      "\n",
      "üîç Data Verification:\n",
      "   Training audio shape: (48000,)\n",
      "   Audio data type: <class 'numpy.ndarray'>\n",
      "   Train: 274 footstep, 833 non-footstep\n",
      "   Val: 48 footstep, 189 non-footstep\n",
      "   Test: 60 footstep, 169 non-footstep\n"
     ]
    }
   ],
   "source": [
    "print(\"üìÇ Loading leak-free datasets...\")\n",
    "\n",
    "# Load the fixed datasets\n",
    "train_df = pd.read_pickle('train_manifest_FIXED.pkl')\n",
    "val_df = pd.read_pickle('val_manifest_FIXED.pkl')\n",
    "test_df = pd.read_pickle('test_manifest_FIXED.pkl')\n",
    "\n",
    "print(f\"‚úÖ Loaded leak-free datasets:\")\n",
    "print(f\"   Training: {len(train_df)} samples\")\n",
    "print(f\"   Validation: {len(val_df)} samples\")\n",
    "print(f\"   Test: {len(test_df)} samples\")\n",
    "\n",
    "# Verify audio data is present\n",
    "print(f\"\\nüîç Data Verification:\")\n",
    "print(f\"   Training audio shape: {train_df['audio_data'].iloc[0].shape}\")\n",
    "print(f\"   Audio data type: {type(train_df['audio_data'].iloc[0])}\")\n",
    "\n",
    "# Check class distribution\n",
    "for split_name, df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
    "    footstep_count = len(df[df['label'] == 1])\n",
    "    non_footstep_count = len(df[df['label'] == 0])\n",
    "    print(f\"   {split_name}: {footstep_count} footstep, {non_footstep_count} non-footstep\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "468526c3-10b2-40e6-af09-8919a754751c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Feature extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "def extract_mfcc_features(audio, sr=SAMPLE_RATE, n_mfcc=N_MFCC, hop_length=HOP_LENGTH):\n",
    "    \"\"\"Extract MFCC features from audio\"\"\"\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "    return mfccs\n",
    "\n",
    "def extract_mel_spectrogram(audio, sr=SAMPLE_RATE, n_mels=N_MELS, hop_length=HOP_LENGTH):\n",
    "    \"\"\"Extract mel-spectrogram for CNN input\"\"\"\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels, hop_length=hop_length)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "def extract_spectral_features(audio, sr=SAMPLE_RATE, hop_length=HOP_LENGTH):\n",
    "    \"\"\"Extract spectral features\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Spectral centroid\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr, hop_length=hop_length)\n",
    "    features['spectral_centroid'] = np.mean(spectral_centroid)\n",
    "    features['spectral_centroid_std'] = np.std(spectral_centroid)\n",
    "    \n",
    "    # Spectral rolloff\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr, hop_length=hop_length)\n",
    "    features['spectral_rolloff'] = np.mean(spectral_rolloff)\n",
    "    features['spectral_rolloff_std'] = np.std(spectral_rolloff)\n",
    "    \n",
    "    # Spectral bandwidth\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr, hop_length=hop_length)\n",
    "    features['spectral_bandwidth'] = np.mean(spectral_bandwidth)\n",
    "    features['spectral_bandwidth_std'] = np.std(spectral_bandwidth)\n",
    "    \n",
    "    # Zero crossing rate\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=audio, hop_length=hop_length)\n",
    "    features['zcr'] = np.mean(zcr)\n",
    "    features['zcr_std'] = np.std(zcr)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_temporal_features(audio, sr=SAMPLE_RATE):\n",
    "    \"\"\"Extract temporal features\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # RMS energy\n",
    "    rms = librosa.feature.rms(y=audio)\n",
    "    features['rms'] = np.mean(rms)\n",
    "    features['rms_std'] = np.std(rms)\n",
    "    \n",
    "    # Tempo (if detectable)\n",
    "    try:\n",
    "        tempo, _ = librosa.beat.beat_track(y=audio, sr=sr)\n",
    "        features['tempo'] = tempo\n",
    "    except:\n",
    "        features['tempo'] = 120.0  # Default tempo\n",
    "    \n",
    "    return features\n",
    "\n",
    "def process_dataset_features(df, split_name):\n",
    "    \"\"\"Extract features from entire dataset\"\"\"\n",
    "    print(f\"üîß Extracting features from {split_name} set ({len(df)} samples)...\")\n",
    "    \n",
    "    mfcc_features = []\n",
    "    mel_features = []\n",
    "    spectral_features = []\n",
    "    temporal_features = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            # Get audio data\n",
    "            audio = np.array(row['audio_data'], dtype=np.float32)\n",
    "            \n",
    "            # Ensure correct length\n",
    "            target_length = int(SAMPLE_RATE * DURATION)\n",
    "            if len(audio) != target_length:\n",
    "                if len(audio) < target_length:\n",
    "                    audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "                else:\n",
    "                    audio = audio[:target_length]\n",
    "            \n",
    "            # Extract features\n",
    "            mfcc = extract_mfcc_features(audio)\n",
    "            mel_spec = extract_mel_spectrogram(audio)\n",
    "            spectral = extract_spectral_features(audio)\n",
    "            temporal = extract_temporal_features(audio)\n",
    "            \n",
    "            mfcc_features.append(mfcc)\n",
    "            mel_features.append(mel_spec)\n",
    "            spectral_features.append(spectral)\n",
    "            temporal_features.append(temporal)\n",
    "            labels.append(row['label'])\n",
    "            \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"   Processed {idx + 1}/{len(df)} samples...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error processing sample {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    features = {\n",
    "        'mfcc': np.array(mfcc_features),\n",
    "        'mel_spectrogram': np.array(mel_features),\n",
    "        'spectral': spectral_features,  # Keep as list of dicts for now\n",
    "        'temporal': temporal_features,  # Keep as list of dicts for now\n",
    "        'labels': np.array(labels)\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {split_name} feature extraction completed!\")\n",
    "    print(f\"   MFCC: {features['mfcc'].shape}\")\n",
    "    print(f\"   Mel-spectrogram: {features['mel_spectrogram'].shape}\")\n",
    "    print(f\"   Spectral: {len(features['spectral'])} samples\")\n",
    "    print(f\"   Temporal: {len(features['temporal'])} samples\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"‚úÖ Feature extraction functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb626fd2-a321-452d-9ef6-10bb9cfc0165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéµ TRAINING SET FEATURE EXTRACTION\n",
      "========================================\n",
      "üîß Extracting features from Training set (1107 samples)...\n",
      "   Processed 100/1107 samples...\n",
      "   Processed 200/1107 samples...\n",
      "   Processed 300/1107 samples...\n",
      "   Processed 400/1107 samples...\n",
      "   Processed 500/1107 samples...\n",
      "   Processed 600/1107 samples...\n",
      "   Processed 700/1107 samples...\n",
      "   Processed 800/1107 samples...\n",
      "   Processed 900/1107 samples...\n",
      "   Processed 1000/1107 samples...\n",
      "   Processed 1100/1107 samples...\n",
      "‚úÖ Training feature extraction completed!\n",
      "   MFCC: (1107, 13, 94)\n",
      "   Mel-spectrogram: (1107, 128, 94)\n",
      "   Spectral: 1107 samples\n",
      "   Temporal: 1107 samples\n",
      "\n",
      "üìä Training Feature Quality:\n",
      "   MFCC mean magnitude: 39.091 (should be > 1.0)\n",
      "   Labels: [833 274]\n"
     ]
    }
   ],
   "source": [
    "print(\"üéµ TRAINING SET FEATURE EXTRACTION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "train_features = process_dataset_features(train_df, \"Training\")\n",
    "\n",
    "# Quick quality check\n",
    "mfcc_mean = np.mean(np.abs(train_features['mfcc']))\n",
    "print(f\"\\nüìä Training Feature Quality:\")\n",
    "print(f\"   MFCC mean magnitude: {mfcc_mean:.3f} (should be > 1.0)\")\n",
    "print(f\"   Labels: {np.bincount(train_features['labels'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b474bfd-9c65-477f-a530-dd4d46752cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéµ VALIDATION SET FEATURE EXTRACTION\n",
      "========================================\n",
      "üîß Extracting features from Validation set (237 samples)...\n",
      "   Processed 100/237 samples...\n",
      "   Processed 200/237 samples...\n",
      "‚úÖ Validation feature extraction completed!\n",
      "   MFCC: (237, 13, 94)\n",
      "   Mel-spectrogram: (237, 128, 94)\n",
      "   Spectral: 237 samples\n",
      "   Temporal: 237 samples\n",
      "\n",
      "üìä Validation Feature Quality:\n",
      "   MFCC mean magnitude: 40.069\n",
      "   Labels: [189  48]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüéµ VALIDATION SET FEATURE EXTRACTION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "val_features = process_dataset_features(val_df, \"Validation\")\n",
    "\n",
    "print(f\"\\nüìä Validation Feature Quality:\")\n",
    "print(f\"   MFCC mean magnitude: {np.mean(np.abs(val_features['mfcc'])):.3f}\")\n",
    "print(f\"   Labels: {np.bincount(val_features['labels'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7330d145-1e73-4bc5-b4cf-fb511a39115f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéµ TEST SET FEATURE EXTRACTION\n",
      "========================================\n",
      "üîß Extracting features from Test set (229 samples)...\n",
      "   Processed 100/229 samples...\n",
      "   Processed 200/229 samples...\n",
      "‚úÖ Test feature extraction completed!\n",
      "   MFCC: (229, 13, 94)\n",
      "   Mel-spectrogram: (229, 128, 94)\n",
      "   Spectral: 229 samples\n",
      "   Temporal: 229 samples\n",
      "\n",
      "üìä Test Feature Quality:\n",
      "   MFCC mean magnitude: 41.026\n",
      "   Labels: [169  60]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüéµ TEST SET FEATURE EXTRACTION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "test_features = process_dataset_features(test_df, \"Test\")\n",
    "\n",
    "print(f\"\\nüìä Test Feature Quality:\")\n",
    "print(f\"   MFCC mean magnitude: {np.mean(np.abs(test_features['mfcc'])):.3f}\")\n",
    "print(f\"   Labels: {np.bincount(test_features['labels'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fee97db-7f6a-4d72-8722-e855437da899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß FEATURE NORMALIZATION\n",
      "==============================\n",
      "   Normalizing MFCC features...\n",
      "   Normalizing Mel-spectrogram features...\n",
      "   Normalizing spectral features...\n",
      "   Normalizing temporal features...\n",
      "\n",
      "‚úÖ Feature normalization completed!\n",
      "   MFCC shapes: Train (1107, 13, 94), Val (237, 13, 94), Test (229, 13, 94)\n",
      "   Mel-spec shapes: Train (1107, 128, 94), Val (237, 128, 94), Test (229, 128, 94)\n",
      "   Spectral shapes: Train (1107, 8)\n",
      "   Temporal shapes: Train (1107, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß FEATURE NORMALIZATION\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "def safe_normalize_features(train_features, val_features, test_features):\n",
    "    \"\"\"Normalize features using training set statistics\"\"\"\n",
    "    normalized_features = {}\n",
    "    scalers = {}\n",
    "    \n",
    "    # 1. Normalize MFCC features\n",
    "    print(\"   Normalizing MFCC features...\")\n",
    "    train_mfcc_flat = train_features['mfcc'].reshape(len(train_features['mfcc']), -1)\n",
    "    val_mfcc_flat = val_features['mfcc'].reshape(len(val_features['mfcc']), -1)\n",
    "    test_mfcc_flat = test_features['mfcc'].reshape(len(test_features['mfcc']), -1)\n",
    "    \n",
    "    mfcc_scaler = StandardScaler()\n",
    "    train_mfcc_norm = mfcc_scaler.fit_transform(train_mfcc_flat)\n",
    "    val_mfcc_norm = mfcc_scaler.transform(val_mfcc_flat)\n",
    "    test_mfcc_norm = mfcc_scaler.transform(test_mfcc_flat)\n",
    "    \n",
    "    normalized_features['train_mfcc'] = train_mfcc_norm.reshape(train_features['mfcc'].shape)\n",
    "    normalized_features['val_mfcc'] = val_mfcc_norm.reshape(val_features['mfcc'].shape)\n",
    "    normalized_features['test_mfcc'] = test_mfcc_norm.reshape(test_features['mfcc'].shape)\n",
    "    scalers['mfcc'] = mfcc_scaler\n",
    "    \n",
    "    # 2. Normalize Mel-spectrograms\n",
    "    print(\"   Normalizing Mel-spectrogram features...\")\n",
    "    mel_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    train_mel_flat = train_features['mel_spectrogram'].reshape(len(train_features['mel_spectrogram']), -1)\n",
    "    val_mel_flat = val_features['mel_spectrogram'].reshape(len(val_features['mel_spectrogram']), -1)\n",
    "    test_mel_flat = test_features['mel_spectrogram'].reshape(len(test_features['mel_spectrogram']), -1)\n",
    "    \n",
    "    train_mel_norm = mel_scaler.fit_transform(train_mel_flat)\n",
    "    val_mel_norm = mel_scaler.transform(val_mel_flat)\n",
    "    test_mel_norm = mel_scaler.transform(test_mel_flat)\n",
    "    \n",
    "    normalized_features['train_mel'] = train_mel_norm.reshape(train_features['mel_spectrogram'].shape)\n",
    "    normalized_features['val_mel'] = val_mel_norm.reshape(val_features['mel_spectrogram'].shape)\n",
    "    normalized_features['test_mel'] = test_mel_norm.reshape(test_features['mel_spectrogram'].shape)\n",
    "    scalers['mel'] = mel_scaler\n",
    "    \n",
    "    # 3. Normalize spectral features\n",
    "    print(\"   Normalizing spectral features...\")\n",
    "    train_spectral_df = pd.DataFrame(train_features['spectral'])\n",
    "    val_spectral_df = pd.DataFrame(val_features['spectral'])\n",
    "    test_spectral_df = pd.DataFrame(test_features['spectral'])\n",
    "    \n",
    "    spectral_scaler = StandardScaler()\n",
    "    train_spectral_norm = spectral_scaler.fit_transform(train_spectral_df)\n",
    "    val_spectral_norm = spectral_scaler.transform(val_spectral_df)\n",
    "    test_spectral_norm = spectral_scaler.transform(test_spectral_df)\n",
    "    \n",
    "    normalized_features['train_spectral'] = train_spectral_norm\n",
    "    normalized_features['val_spectral'] = val_spectral_norm\n",
    "    normalized_features['test_spectral'] = test_spectral_norm\n",
    "    scalers['spectral'] = spectral_scaler\n",
    "    \n",
    "    # 4. Normalize temporal features\n",
    "    print(\"   Normalizing temporal features...\")\n",
    "    train_temporal_df = pd.DataFrame(train_features['temporal'])\n",
    "    val_temporal_df = pd.DataFrame(val_features['temporal'])\n",
    "    test_temporal_df = pd.DataFrame(test_features['temporal'])\n",
    "    \n",
    "    temporal_scaler = StandardScaler()\n",
    "    train_temporal_norm = temporal_scaler.fit_transform(train_temporal_df)\n",
    "    val_temporal_norm = temporal_scaler.transform(val_temporal_df)\n",
    "    test_temporal_norm = temporal_scaler.transform(test_temporal_df)\n",
    "    \n",
    "    normalized_features['train_temporal'] = train_temporal_norm\n",
    "    normalized_features['val_temporal'] = val_temporal_norm\n",
    "    normalized_features['test_temporal'] = test_temporal_norm\n",
    "    scalers['temporal'] = temporal_scaler\n",
    "    \n",
    "    # Store labels\n",
    "    normalized_features['train_labels'] = train_features['labels']\n",
    "    normalized_features['val_labels'] = val_features['labels']\n",
    "    normalized_features['test_labels'] = test_features['labels']\n",
    "    \n",
    "    return normalized_features, scalers\n",
    "\n",
    "# Apply normalization\n",
    "normalized_features, scalers = safe_normalize_features(train_features, val_features, test_features)\n",
    "\n",
    "print(f\"\\n‚úÖ Feature normalization completed!\")\n",
    "print(f\"   MFCC shapes: Train {normalized_features['train_mfcc'].shape}, Val {normalized_features['val_mfcc'].shape}, Test {normalized_features['test_mfcc'].shape}\")\n",
    "print(f\"   Mel-spec shapes: Train {normalized_features['train_mel'].shape}, Val {normalized_features['val_mel'].shape}, Test {normalized_features['test_mel'].shape}\")\n",
    "print(f\"   Spectral shapes: Train {normalized_features['train_spectral'].shape}\")\n",
    "print(f\"   Temporal shapes: Train {normalized_features['train_temporal'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "893092e2-5b52-4df8-9310-1aaff56bde89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ SAVING PROCESSED FEATURES (LEAK-FREE)\n",
      "========================================\n",
      "‚úÖ Saved complete feature set to 'extracted_features_FIXED.pkl'\n",
      "‚úÖ Saved individual feature matrices (FIXED versions)\n",
      "\n",
      "üéâ PHASE 2 COMPLETED (LEAK-FREE)!\n",
      "   Ready for Phase 3: CNN Training with realistic performance\n",
      "   Expected accuracy: 75-85% (no more 100% fake scores)\n"
     ]
    }
   ],
   "source": [
    "print(\"üíæ SAVING PROCESSED FEATURES (LEAK-FREE)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Save all features and scalers\n",
    "features_to_save = {\n",
    "    'normalized_features': normalized_features,\n",
    "    'scalers': scalers,\n",
    "    'config': {\n",
    "        'sample_rate': SAMPLE_RATE,\n",
    "        'duration': DURATION,\n",
    "        'n_mfcc': N_MFCC,\n",
    "        'n_mels': N_MELS,\n",
    "        'hop_length': HOP_LENGTH,\n",
    "        'n_fft': N_FFT\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to pickle file\n",
    "with open('extracted_features_FIXED.pkl', 'wb') as f:\n",
    "    pickle.dump(features_to_save, f)\n",
    "\n",
    "print(f\"‚úÖ Saved complete feature set to 'extracted_features_FIXED.pkl'\")\n",
    "\n",
    "# Save individual feature matrices\n",
    "np.save('train_mfcc_FIXED.npy', normalized_features['train_mfcc'])\n",
    "np.save('val_mfcc_FIXED.npy', normalized_features['val_mfcc'])\n",
    "np.save('test_mfcc_FIXED.npy', normalized_features['test_mfcc'])\n",
    "\n",
    "np.save('train_mel_spec_FIXED.npy', normalized_features['train_mel'])\n",
    "np.save('val_mel_spec_FIXED.npy', normalized_features['val_mel'])\n",
    "np.save('test_mel_spec_FIXED.npy', normalized_features['test_mel'])\n",
    "\n",
    "np.save('train_labels_FIXED.npy', normalized_features['train_labels'])\n",
    "np.save('val_labels_FIXED.npy', normalized_features['val_labels'])\n",
    "np.save('test_labels_FIXED.npy', normalized_features['test_labels'])\n",
    "\n",
    "print(f\"‚úÖ Saved individual feature matrices (FIXED versions)\")\n",
    "\n",
    "print(f\"\\nüéâ PHASE 2 COMPLETED (LEAK-FREE)!\")\n",
    "print(f\"   Ready for Phase 3: CNN Training with realistic performance\")\n",
    "print(f\"   Expected accuracy: 75-85% (no more 100% fake scores)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9774120e-4e5b-49be-9bc9-996579d09de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134aca9c-0047-4e22-a824-572d31a54b90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
